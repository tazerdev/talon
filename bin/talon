#!/usr/bin/env python3

import os
import csv
import sys
import json
import time
import concurrent.futures
from multiprocessing import Pool
from itertools import product

from signal import signal, SIGINT, SIG_IGN
import argparse
import talonlib
from pathlib import Path
import datetime
from datetime import datetime as dt
from datetime import timedelta
from datetime import time as dtt
from zoneinfo import ZoneInfo

def signal_handler(signum, frame):
    print("CTRL-C detected, exiting.")

    signal(signum, SIG_IGN)
    sys.exit(0)

def json_dt_serialize(obj):
    if isinstance(obj, (dt, datetime.date)):
        return obj.isoformat()

    raise TypeError(f"Type {type(obj)} not serializable")

def ParseCommandLineArguments():
    arg_parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, description="tlist - A utility for listing details of WAV files.")
    arg_parser.add_argument('-p', '--path', default=".", type=str, help="Directory to parse.")
    arg_parser.add_argument('file', nargs='?', help="File to parse, if no file is specified then all files in the current directory will be considered.")
    arg_parser.add_argument('-c', '--config', default="talon.ini", type=str, help="Path to a Talon config file to attempt to read.")
    arg_parser.add_argument('-j', '--json', action='store_true', help='Output results in JSON format.')
    arg_parser.add_argument('-d', '--debug', action='store_true', help='Print extra debugging information.')
    arg_parser.add_argument('-r', '--recurse', action='store_true', help='List files recursively.')
    arg_parser.add_argument('-s', '--species', default="all", nargs='?', help="Common name of a bird (e.g., woothr, Barn Owl, etc) you wish to work with.")
    arg_parser.add_argument('-e', '--engine', default='all', nargs='+', choices=['all', 'bn', 'nh', 'ta'], help="The analysis engine whose results you wish to return.")
    arg_parser.add_argument('-o', '--protocol', default='all', nargs='+', choices=['all', 'nfc', 'noc', 'day',], help="The eBird protocol whose results you wish to return.")
    # arg_parser.add_argument('--disposition', default='all', nargs='+', choices=['all', 'confirmed', 'excluded', 'unconfirmed',], help="The eBird protocol whose results you wish to return.")

    arg_parser.add_argument('-t', '--threshold', default=0.25, type=float, help="Only generate output for events above this confidence threshold.")
    arg_parser.add_argument('--station', default='all', type=str, help="The talon audio station.")

    arg_parser.add_argument('--start', type=dt.fromisoformat, help="A datetime in ISO format.", default=None)
    arg_parser.add_argument('--duration', default=None, type=float, help="A duration in hours.")
    arg_parser.add_argument('--clip', action='store_true', help="Extract audio clips of results.")
    arg_parser.add_argument('--graph', action='store_true', help="Generate a spectrograph for the extracted audio clip (implies --clip).")
    arg_parser.add_argument('--force', action='store_true', help="Overwrite audio clips.")
    arg_parser.add_argument('--full-height', action='store_true', help="Set this option if you want the entire frequency to be graphed, otherwise the audio will be resampled to 22,050Hz.")
    arg_parser.add_argument('--timeseries', action='store_true', help="Extract audio clips of results.")
    arg_parser.add_argument('--checklist', action='store_true', help="Create checklists.")
    arg_parser.add_argument('--cleanup', action='store_true', help="Clean up clips and graph for displayed detections.")
    arg_parser.add_argument('-l', '--location', type=str, help="The name of a location specified in the config file, for use with checklists.")

    arg_parser.add_argument('--disposition', default='all', nargs='+', choices=['all', 'confirmed', 'unconfirmed', 'excluded'], help="The analysis engine whose results you wish to return.")
    arg_parser.add_argument('--add-detection', default=None, type=str, help="Add a manual detection in a CSV format with these fields in this order: (WAV file, start time, stop time, species code, probability, disposition) Example: 'DR10L_20251101_005921-0400.WAV,105,108,uplsan,.75,unconfirmed'")
    arg_parser.add_argument('--update-detection', default=None, type=str, help="Add an entry to a CSV format with these fields in this order: (WAV file, start time, stop time, species code, probability, disposition) Example: 'DR10L_20251101_005921-0400.WAV,105,108,uplsan,.75,unconfirmed'")
    return arg_parser

def events_worker(twf, events):
    twf.GetEvents()
    # twf.SaveEvents()
    events += twf.metadata['events']

def twf_worker(filename, section, taxonomy, found_files, startdt=None):
    start = dt.strptime(str(os.path.basename(filename)), section['file_format'])
    curfile = talonlib.TalonWAVFile(filename, section, taxonomy)

    if startdt:
        dur = curfile.metadata['duration']
        stop = start + timedelta(seconds=dur)
        cur = startdt

    if not startdt or (start <= cur < stop) or (start > cur):
        found_files[curfile.metadata['name']] = curfile

def extract_worker(twf, event, graph, force):
    try:
        twf.extract_audio(event, 3, 'clips', False, graph, force)
    except KeyboardInterrupt:
        dur_sec = event['stop'] - event['start']
        print(f"Terminating worker for {event['dt'].strftime('%Y-%m-%d-%H:%M:%S')} - {(event['dt']+timedelta(seconds=dur_sec)).strftime('%Y-%m-%d-%H:%M:%S')}")

def add_detection(user_string, found_files):
    # TODO: validate cur[0] == str, cur[1:2] == int, cur[3] == str, cur[4] == int or float, cur[5] == str
    # example user string
    # "DR10L_20251101_005921-0400.WAV,105,108,uplsan,.75,unconfirmed"
    cur = user_string.split(',')

    dest_path, filename = os.path.split(cur[0])
    filename, extension = os.path.splitext(filename)
    ta_file = os.path.join(dest_path, str(filename) + '_talon.csv')
    ta_fields = [ 'filename', 'orig_start', 'orig_end', 'orig_engine', 'orig_species', 'start', 'end', 'species_code', 'probability', 'disposition' ]

    # filename,orig_start,orig_end,orig_engine,orig_species,start,end,species_code,probability,disposition
    row = [ cur[0], 0, 0, '', '', cur[1], cur[2], cur[3], cur[4], cur[5] ]

    print(f"Writing contents to {ta_file}: {row}")

    # if file exists, append, otherwise create a new csv with header
    if os.path.exists(ta_file):
        # TODO: should attempt to read the file first and then verify that the row doesn't already exist
        with open(ta_file, mode='a', newline='') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(row)
    else:
        with open(ta_file, "w", newline="") as csvfile:
            # writer = csv.writer(csvfile, fieldnames=ta_fields)
            writer.writerow(ta_fields)
            writer.writerow(row)

def update_detection(user_string, events):
    # example user string
    # "2025-12-23-08:59:57-0500,Downy Woodpecker,confirmed"
    cur = user_string.split(',')
    row = []

    ta_fields = [ 'filename', 'orig_start', 'orig_end', 'orig_engine', 'orig_species', 'start', 'end', 'species_code', 'probability', 'disposition' ]

    for ev in events:
        if dt.fromisoformat(cur[0]) == ev['dt'] and cur[1] == ev['common_name']:
            row = [ ev['filename'], ev['start'], ev['stop'], ev['engine'], ev['species_code'], ev['start'], ev['stop'], ev['species_code'], ev['probability'], cur[2] ]

            dest_path, filename = os.path.split(ev['filename'])
            filename, extension = os.path.splitext(filename)
            ta_file = os.path.join(dest_path, str(filename) + '_talon.csv')

            # if file exists, append, otherwise create a new csv with header
            if os.path.exists(ta_file):
                # TODO: should attempt to read the file first and then verify that the row doesn't already exist
                with open(ta_file, mode='a', newline='') as csvfile:
                    writer = csv.writer(csvfile)
                    writer.writerow(row)
            else:
                with open(ta_file, "w", newline="") as csvfile:
                    writer = csv.writer(csvfile, fieldnames=ta_fields)
                    writer.writeheader()
                    writer.writerow(row)

def generate_timeseries_graph(events: list, frequency=5, ymax=None):
    """
    Generate a frequency graph of supplied events.

    Args:
        events (list): A list of event dicts which we'll use to generate the graph.
        start_dt (datetime): The start time of data we wish to graph.
        stop_dt (datetime): The stop time of data we wish to graph.
        frequency (int): The time duration (minutes) we'll lump events into for the graph. 
        title (str): The title of the graph.
        ymax (int): Maximum height of the chart (minimum is the bucket with the most events).
        tz (str): The time zone of the data.
    """
    import math
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.ticker as tck
    import matplotlib.dates as mdates

    matplotlib.use('agg')

    rollavg = []
    buckets = []
    totals = []
    labels = []

    start_dt = events[0]['dt']
    stop_dt = events[-1]['dt']
    
    title = ''

    if ymax is None:
        max_value = 0
    else:
        max_value = ymax

    print(events, frequency, ymax)
    # create the frequency buckets before we iterate events
    if frequency > 0:
        # print(f"(({stop_dt} - {start_dt}).total_seconds() / 60) / {frequency}")
        # print((stop_dt - start_dt).total_seconds())
        start = start_dt.replace(minute=0, second=0, microsecond=0)
        stop = stop_dt.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1, minutes=frequency)

        numbuckets = int(((stop - start).total_seconds() / 60) / frequency)
        interval = timedelta(minutes=frequency)

        # zero out start date so we can start on the hour and have our
        # x-axis labels look nice
        # cur = start_dt.replace(minute=0, second=0, microsecond=0)

        # initialize buckets
        for i in range(0, numbuckets):
            buckets.append(start)
            totals.append(0)

            tmp = start + interval

            for event in events:
                # if DEBUG:
                #     print(f"({event['dt']} >= {start})={event['dt'] >= start} and ){event['dt']} < {tmp})={event['dt'] < tmp}")

                if event['dt'] >= start and event['dt'] < tmp:
                    totals[i] += 1

            start = tmp

        # add datetime labels
        for bucket in buckets:
            labels.append(bucket)
            # print(bucket)

        # if DEBUG:
        #     print(f"numbuckets: {numbuckets}, interval: {interval}")

        #     for i in range(0, numbuckets):
        #         print(labels[i], totals[i])

    for i in range(numbuckets):
        # handle averaging the starting and ending buckets by making them 0
        if i == 0 or i == numbuckets-1:
            rollavg.append(0)
        else:
            rollavg.append((totals[i-1] + totals[i] + totals[i+1])/3)

    for i in rollavg:
        if i > max_value:
            max_value = math.ceil(i / 10.0) * 10

    title += f"{frequency * 3}-Minute Rolling Average"
    title += f"\n{buckets[0].strftime('%m/%d/%Y %H:%M:%S%z')} - {buckets[-1].strftime('%m/%d/%Y %H:%M:%S%z')}"

    fig, ax = plt.subplots(figsize=(32, 18))
    plt.grid()

    plt.ylim(0, max_value)

    # This trims the x-axis, which we don't necessarily want. May need
    # to turn this into an option (e.g., --x-trim)
    # plt.xlim(buckets[0], buckets[-1])
    plt.rcParams['timezone'] = 'US/Eastern'

    # Format the x-axis labels as dates
    # date_format = mdates.DateFormatter("%H:%M:%S", tz=pytz.timezone(tz))
    date_format = mdates.DateFormatter("%H:%M:%S")
    ax.xaxis.set_major_formatter(date_format)

    # TODO: For long time periods, switch to using hourlocator instead,
    # maybe try and keep the x-labels fewer than 30.
    # ax.xaxis.set_major_locator(mdates.HourLocator(byhour=2,interval=1))
    ax.xaxis.set_major_locator(mdates.MinuteLocator(byminute=[0,30]))

    plt.xlabel('Time', fontsize=24, labelpad=30)
    plt.ylabel('Detections', fontsize=24, labelpad=30)
    plt.title(title, fontsize=32, pad=30)
    plt.xticks(fontsize=16, rotation=60)
    plt.yticks(fontsize=16)
    plt.plot(labels, rollavg)
    plt.fill_between(labels, rollavg, alpha=0.3)

    plt.savefig('timeseries.png', pad_inches=0.5, bbox_inches='tight')
    plt.close()

def generate_checklists(config, section, events):
    # TODO: Warning, there's the risk of getting the dates/times wrong
    # as we're blindly casing the datetimes of the events as the time
    # zone supplied in the section. For now it's up to the user to not
    # mix up events from different locations.

    # start at midnight of the date of the first event, just so we
    # are sure our checklist windows are full hours
    curtz = ZoneInfo(config[section]['timezone'])
    start_date = dt.combine(events[0]['dt'].date(), dtt(0,0,0)).replace(microsecond=0)
    
    statmd = f"{section}.metadata"

    if statmd in config and 'name' in config[statmd]:
        location = config[statmd]['name']
    else:
        location = f"{float(config[section]['latitude']):5.8f}, {float(config[section]['longitude']):5.8f}"

    ts = talonlib.TalonSchedule(
        config[section]['latitude'],
        config[section]['longitude'],
        config[section]['timezone'],
        location,
        start_date
    )

    checklists = []

    for window in ts:
        if window['start'] > events[-1]['dt'].astimezone(curtz):
            break
        else:
            curcheck = {}

            curcheck['start'] = window['start'].replace(microsecond=0)
            curcheck['stop'] = window['stop'].replace(microsecond=0)
            curcheck['protocol'] = window['protocol']
            curcheck['species'] = {}

            for evt in events:
                if window['start'] <= evt['dt'].astimezone(curtz) < window['stop']:
                    name = f"{evt['common_name']} ({evt['species_code']})"

                    if name in curcheck['species']:
                        curcheck['species'][name] += 1
                    else:
                        curcheck['species'][name] = 1
            
            checklists.append(curcheck)

    output = ''
    max = 0
    i = 0

    for ckl in checklists:
        if len(ckl['species']) > 0:
            max += 1

    for ckl in checklists:
        if len(ckl['species']) > 0:
            i += 1
            total_individuals = 0

            duration = ckl['stop'] - ckl['start']

            for species in ckl['species']:
                total_individuals += ckl['species'][species]

            output += '*' * 42 + '\n'
            output += f"* Checklist: {i} of {max}\n"
            output += '*' * 42 + '\n'
            output += f"Start Time     : {ckl['start']}\n"
            output += f"Stop Time      : {ckl['stop']}\n"
            output += f"Duration       : {duration}\n"
            output += f"Location       : {location}\n"
            output += f"eBird Protocol : {ckl['protocol']}\n"
            # output += f"Total Species  : {len(ckl['species'])}\n"
            output += f"Total Calls    : {total_individuals}\n"
            output += '-' * 42 + '\n'

            # sort species in alphabetical order
            spc = sorted(ckl['species'], key=lambda d: d)

            for species in spc:
                output += f"{ckl['species'][species]:>2} - {species}\n"

            output += '\n'
    
    return output

def cleanup(clip_dir, events):
    for event in events:
        clip_file = os.path.join(clip_dir, f"{event['dt'].strftime('%Y%m%d-%H%M%S%z')}-{event['engine']}-{event['species_code']}.WAV")
        spec_file = os.path.join(clip_dir, f"{event['dt'].strftime('%Y%m%d-%H%M%S%z')}-{event['engine']}-{event['species_code']}.PNG")

        try:
            if os.path.exists(clip_file):
                print(f"Removing {clip_file}")
                os.remove(clip_file)
        except PermissionError as e:
            print(f"Permission error removing {clip_file}")

        try:
            if os.path.exists(spec_file):
                print(f"Removing {spec_file}")
                os.remove(spec_file)
        except PermissionError as e:
            print(f"Permission error removing {spec_file}")

def main():
    signal(SIGINT, signal_handler)

    RED = '\033[31m'
    GREEN = '\033[32m'
    CYAN = '\033[36m'
    RESET = '\033[0m'

    arg_parser = ParseCommandLineArguments()
    args = arg_parser.parse_args()

    if not os.path.exists(args.config):
        print(f"Configuration file not found: {args.config}, exiting.")
        sys.exit(-1)

    tc = talonlib.TalonConfig(args.config)
    config = tc.config

    sfilter = {}
    taxonomy = {}
    groupcodexref = {}

    find_result = []
    events = []

    globstr = "*.[wW][aA][vV]"

    found_files = {}

    output = ""
    mp_twf_list = []

    maxid = 0
    maxstation = 0

    last_section = ""

    if args.add_detection:
        add_detection(args.add_detection, found_files)
        sys.exit(0)

    # get the parent directory of the bin dir
    talon_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    fpath = os.path.join(talon_dir, 'filter.csv')

    # load the species filter, taxonomy, and group code xref
    if os.path.exists(fpath):
        with open(fpath, mode='r') as infile:
            reader = csv.reader(infile)
            sfilter = { row[0]:float(row[1]) for row in reader }

    if 'taxonomy' in config and 'ebird_taxonomy' in config['taxonomy']:
        epath = os.path.join(talon_dir, config['taxonomy']['ebird_taxonomy'])

        if os.path.exists(epath):
            with open(epath, mode='r') as infile:
                reader = csv.reader(infile)
                taxonomy = { row[2]:row[4] for row in reader }

    if 'taxonomy' in config and 'group_code_xref' in config['taxonomy']:
        gpath = os.path.join(talon_dir, config['taxonomy']['group_code_xref'])

        if os.path.exists(gpath):
            with open(gpath, mode='r') as infile:
                reader = csv.reader(infile)
                groupcodexref = { row[0]:row[1] for row in reader }

    # combine taxonomy and groupcodexref so we can render nighthawk
    # group codes as standard eBird 'spuhs'
    taxonomy = taxonomy | groupcodexref

    if not args.file:
        # Can only iterate a generator once, so we'll convert it to a list
        if args.recurse:
            find_result = list(Path(args.path).rglob(globstr))
        else:
            find_result = list(Path(args.path).glob(globstr))
    else:
        find_result = [args.file]

    # spinning off twf creation into separate threads improves performance
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        for filename in find_result:
            for section in config:
                if 'type' in config[section] and config[section]['type'] == 'station':
                    try:
                        if str(filename).split('_')[0] == section:
                            last_section = section
                            future = executor.submit(twf_worker, filename, config[section], taxonomy, found_files, args.start)
                    except ValueError as e:
                        pass

    # spinning off event gathering into separate threads improves performance
    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
        for file in found_files:
            future = executor.submit(events_worker, found_files[file], events)

    # if an identity was specified we don't need to waste time loading the filter
    # table from disk, just limit to what the user supplied
    if args.species != 'all':
        events = [d for d in events if d['common_name'] in args.species]

    if args.protocol != 'all':
        events = [d for d in events if d['protocol'] in args.protocol]

    if args.engine != 'all':
        events = [d for d in events if d['engine'] in args.engine]

    if args.station != 'all':
        events = [d for d in events if d['station'] in [args.station]]

    if args.disposition != 'all':
        events = [d for d in events if d['disposition'] in args.disposition]

    events = [d for d in events if d['probability'] >= args.threshold]

    events = [d for d in events if d['common_name'] not in sfilter or d['probability'] > sfilter[d['common_name']]]
    # events = [d for d in events if d['species_code'] not in sfilter or d['probability'] > sfilter[d['species_code']]]

    # sort all events by date
    events = sorted(events, key=lambda d: d['dt'])

    if events:
        if args.start:
            # TODO: Figure out a better way of determining the appropriate timezone
            window_start = args.start.astimezone(events[0]['dt'].tzinfo)

            if not args.duration:
                duration = timedelta(hours=24)
            else:
                duration = timedelta(hours=args.duration)

            window_end = window_start + duration

        if args.update_detection:
            update_detection(args.update_detection, events)

        if args.json:
            print(json.dumps(events, default=json_dt_serialize, indent=4, sort_keys=True))
            sys.exit(0)

        for ev in events:
            # if user didn't specify a start date, or if they did and the event falls within the
            # calculated range (start + duration)
            if not args.start or (ev['dt'] >= window_start and ev['dt'] <= window_end):
                if ev['probability'] > args.threshold and (ev['common_name'] not in sfilter or ev['probability'] > sfilter[ev['common_name']]):
                    idlen = len(ev['species_code'])
                    stnlen = len(ev['station'])

                    if idlen > maxid:
                        maxid = idlen

                    if stnlen > maxstation:
                        maxstation = stnlen

        # the main part of the app, iterate all events filtering for
        # only the events that were requested.
        for ev in events:
            # if user didn't specify a start date, or if they did and the event falls within the
            # calculated range (start + duration)
            if not args.start or (ev['dt'] >= window_start and ev['dt'] <= window_end):
                # if ev['probability'] > args.threshold and (ev['common_name'] not in sfilter or ev['probability'] > sfilter[ev['common_name']]):
                idlen = len(ev['species_code'])
                stnlen = len(ev['station'])

                if args.clip or args.graph:
                    start = ev['start']
                    duration = ev['stop'] - start

                    dest_path, filename = os.path.split(ev['filename'])
                    dest_path = os.path.join(dest_path, 'clips')
                    filename, extension = os.path.splitext(filename)

                    if args.clip or args.graph:
                        cur = found_files[ev['filename']]
                        mp_twf_list.append([cur, ev, args.graph, args.force])
                else:
                    if ev['disposition'] == 'confirmed':
                        color = GREEN
                    elif ev['disposition'] == 'excluded':
                        color = RED
                    elif ev['overridden']:
                        color = CYAN
                    else:
                        color = RESET

                    # accepted iso formats:
                    # https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat
                    output += f"{color}{ev['dt'].isoformat(timespec='milliseconds')}  "
                    output += f"{ev['start_rel']}  {ev['station']:{maxstation}}  "
                    output += f"{ev['protocol']:3}  {ev['engine']:2}  "
                    output += f"{ev['probability']*100:>6.2f}%  {ev['common_name']} ({ev['species_code']}){RESET}\n"

    if args.timeseries:
        generate_timeseries_graph(events, frequency=5)

    elif args.checklist:
        if not args.location:
            section = config['general']['default']
        else:
            section = args.location

        if events:
            output = generate_checklists(config, last_section, events)
        else:
            output = "No events to add to a checklist."

        print(output.strip())

    # matplotlib is not thread safe, so we have to use multiprocessing
    # this is very fast, reducing the clip/graph rendering time from an
    # average of 2-3s, to 0.47s. Reducing the core allocation by 1 to
    # give the OS a core to do housekeeping with. Additionally, you can't
    # CTRL-C a worker proc safely in Windows without using the 'with pool'
    # context.
    elif args.clip or args.graph:
        try:
            with Pool(processes=os.cpu_count() - 1) as p:
                res = p.starmap_async(extract_worker, mp_twf_list)

                while not res.ready():
                    time.sleep(1)

                result = res.get()
        except KeyboardInterrupt:
            print('Exiting')
            p.close()
    
        p.join()
    elif args.cleanup:
        if events:
            dest_path, filename = os.path.split(ev['filename'])
            clip_dir = os.path.join(dest_path, 'clips')

            cleanup(clip_dir, events)
    else:
        if len(output) > 0:
            print(output.strip())


if __name__ == "__main__":
    sys.exit(main())
